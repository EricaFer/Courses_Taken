{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Calculate the mean fare_amount on the validation_train data\n",
    "naive_prediction = np.mean(validation_train['fare_amount'])\n",
    "\n",
    "# Assign naive prediction to all the holdout observations\n",
    "validation_test['pred'] = naive_prediction\n",
    "\n",
    "# Measure the local RMSE\n",
    "rmse = sqrt(mean_squared_error(validation_test['fare_amount'], validation_test['pred']))\n",
    "print('Validation RMSE for Baseline I model: {:.3f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pickup hour from the pickup_datetime column\n",
    "train['hour'] = train['pickup_datetime'].dt.hour\n",
    "test['hour'] = test['pickup_datetime'].dt.hour\n",
    "\n",
    "# Calculate average fare_amount grouped by pickup hour \n",
    "hour_groups = train.groupby('hour')['fare_amount'].mean()\n",
    "\n",
    "# Make predictions on the test set\n",
    "test['fare_amount'] = test.hour.map(hour_groups)\n",
    "\n",
    "# Write predictions\n",
    "test[['id','fare_amount']].to_csv('hour_mean_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select only numeric features\n",
    "features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
    "            'dropoff_latitude', 'passenger_count', 'hour']\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(train[features], train.fare_amount)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test['fare_amount'] = rf.predict(test[features])\n",
    "\n",
    "# Write predictions\n",
    "test[['id','fare_amount']].to_csv('rf_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible max depth values\n",
    "max_depth_grid = [3,6,9,12,15]\n",
    "results = {}\n",
    "\n",
    "# For each value in the grid\n",
    "for max_depth_candidate in max_depth_grid:\n",
    "    # Specify parameters for the model\n",
    "    params = {'max_depth': max_depth_candidate}\n",
    "\n",
    "    # Calculate validation score for a particular hyperparameter\n",
    "    validation_score = get_cv_score(train, params)\n",
    "\n",
    "    # Save the results for each max depth value\n",
    "    results[max_depth_candidate] = validation_score   \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Hyperparameter grids\n",
    "max_depth_grid = [3,5,7]\n",
    "subsample_grid = [0.8,0.9,1.0]\n",
    "results = {}\n",
    "\n",
    "'''Apply the product() function from the itertools package to the hyperparameter grids. It returns all possible combinations for these two grids.'''\n",
    "\n",
    "# For each couple in the grid\n",
    "for max_depth_candidate, subsample_candidate in itertools.product(max_depth_grid, subsample_grid):\n",
    "    params = {'max_depth': max_depth_candidate,\n",
    "              'subsample': subsample_candidate}\n",
    "    validation_score = get_cv_score(train, params)\n",
    "    # Save the results for each couple\n",
    "    results[(max_depth_candidate, subsample_candidate)] = validation_score   \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# Train a Gradient Boosting model\n",
    "gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestRegressor().fit(train[features], train.fare_amount)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test['gb_pred'] = gb.predict(test[features])\n",
    "test['rf_pred'] = rf.predict(test[features])\n",
    "\n",
    "# Find mean of model predictions\n",
    "test['blend'] = (test.gb_pred + test.rf_pred) / 2\n",
    "print(test[['gb_pred', 'rf_pred', 'blend']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# Split train data into two parts\n",
    "part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)\n",
    "\n",
    "# Train a Gradient Boosting model on Part 1\n",
    "gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\n",
    "\n",
    "# Train a Random Forest model on Part 1\n",
    "rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)\n",
    "\n",
    "# Make predictions on the Part 2 data\n",
    "part_2['gb_pred'] = gb.predict(part_2[features])\n",
    "part_2['rf_pred'] = rf.predict(part_2[features])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test['gb_pred'] = gb.predict(test[features])\n",
    "test['rf_pred'] = rf.predict(test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create linear regression model without the intercept\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Train 2nd level model on the Part 2 data\n",
    "lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)\n",
    "\n",
    "# Make stacking predictions on the test data\n",
    "test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])\n",
    "\n",
    "# Look at the model coefficients\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of the initial train DataFrame\n",
    "new_train_2 = train.copy()\n",
    "\n",
    "# Find sum of pickup latitude and ride distance\n",
    "new_train_2['weird_feature'] = new_train_2['pickup_latitude'] + new_train_2['distance_km']\n",
    "\n",
    "# Compare validation scores\n",
    "initial_score = get_cv_score(train)\n",
    "new_score = get_cv_score(new_train_2)\n",
    "\n",
    "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
